{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicting Probability of Quitting and Providing Prevention Hints"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Necessary Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "import time\n",
                "import os\n",
                "from collections import Counter\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
                "from xgboost import XGBClassifier, XGBRegressor\n",
                "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
                "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "import statsmodels.api as sm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "import json"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load the Dataset and Check Duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "file_path = 'HR_Dataset.csv'\n",
                "df = pd.read_csv(file_path)\n",
                "\n",
                "# Select only the relevant columns\n",
                "selected_columns = ['satisfaction_level', 'time_spend_company', 'average_montly_hours', 'number_project', 'salary', 'left']\n",
                "df = df[selected_columns]\n",
                "\n",
                "# Identify all duplicates, including their first occurrences\n",
                "all_duplicates = df.duplicated(keep=False)\n",
                "\n",
                "# Filter the DataFrame to show only duplicate rows\n",
                "duplicate_rows_df = df[all_duplicates]\n",
                "\n",
                "# Check if there are any duplicates\n",
                "if not duplicate_rows_df.empty:\n",
                "    # Sort the DataFrame by all columns to help in visually comparing the duplicate entries\n",
                "    sorted_duplicate_rows_df = duplicate_rows_df.sort_values(by=list(df.columns))\n",
                "\n",
                "    # Display the number of additional copies only\n",
                "    # We use df.duplicated() without keep=False to count only the additional copies\n",
                "    additional_copies_count = df.duplicated().sum()\n",
                "    print(f'Number of additional duplicate copies (excluding the first occurrence): {additional_copies_count}')\n",
                "    print('Duplicate Rows (including originals):')\n",
                "    print(sorted_duplicate_rows_df)\n",
                "\n",
                "    # Save to a CSV file if it doesn't exist\n",
                "    csv_file_path = 'duplicate_rows.csv'\n",
                "    if not os.path.exists(csv_file_path):\n",
                "        sorted_duplicate_rows_df.to_csv(csv_file_path, index=False)\n",
                "        print(f'All duplicate rows (including originals) have been saved to \"{csv_file_path}\".')\n",
                "    else:\n",
                "        print(f'The file \"{csv_file_path}\" already exists.')\n",
                "\n",
                "    # Remove all duplicate rows, keeping only the first occurrence\n",
                "    df = df.drop_duplicates()\n",
                "\n",
                "    # Confirm the removal\n",
                "    print(f'Number of rows after removing duplicates: {df.shape[0]}')\n",
                "else:\n",
                "    print('No duplicate rows found.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the leave rate by average monthly hours\n",
                "def plot_left_rate_by_hours(df, bin_width=10):\n",
                "    bins = range(0, df['average_montly_hours'].max() + bin_width, bin_width)\n",
                "    df['hours_bin'] = pd.cut(df['average_montly_hours'], bins=bins)\n",
                "    grouped = df.groupby('hours_bin')['left'].mean()\n",
                "    count = df.groupby('hours_bin')['left'].count()\n",
                "\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    grouped.plot(kind='bar', color='blue', alpha=0.6)\n",
                "    plt.xlabel('Average Monthly Hours')\n",
                "    plt.ylabel('Leave Rate')\n",
                "    plt.title('Leave Rate by Average Monthly Hours')\n",
                "    plt.xticks(rotation=45)\n",
                "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    plt.show()\n",
                "\n",
                "# Plot leave rate by average monthly hours\n",
                "plot_left_rate_by_hours(df)\n",
                "\n",
                "# Plot leave rate by number of projects\n",
                "def plot_left_rate_by_projects(df):\n",
                "    grouped = df.groupby('number_project')['left'].mean()\n",
                "    count = df.groupby('number_project')['left'].count()\n",
                "\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    grouped.plot(kind='bar', color='blue', alpha=0.6)\n",
                "    plt.xlabel('Number of Projects')\n",
                "    plt.ylabel('Leave Rate')\n",
                "    plt.title('Leave Rate by Number of Projects')\n",
                "    plt.xticks(rotation=0)\n",
                "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    plt.show()\n",
                "\n",
                "# Plot leave rate by number of projects\n",
                "plot_left_rate_by_projects(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify numerical and categorical columns\n",
                "numerical_cols = ['satisfaction_level', 'time_spend_company', 'average_montly_hours', 'number_project']\n",
                "ordinal_cols = ['salary']\n",
                "\n",
                "# Define the ordinal encoder for the 'salary' column\n",
                "salary_categories = ['low', 'medium', 'high']\n",
                "ordinal_encoder = OrdinalEncoder(categories=[salary_categories])\n",
                "\n",
                "# Preprocessing pipeline for classification\n",
                "preprocessor_class = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), numerical_cols),\n",
                "        ('ord', ordinal_encoder, ordinal_cols)\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Preprocessing pipeline for regression\n",
                "preprocessor_reg = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), ['average_montly_hours', 'time_spend_company', 'number_project']),\n",
                "        ('ord', ordinal_encoder, ordinal_cols)\n",
                "        # Check if there's a 'nom' transformer and add it if missing\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Define the feature columns and the target column for classification\n",
                "X_class = df.drop(columns=['left'])\n",
                "y_class = df['left']\n",
                "\n",
                "# Define the feature columns and the target column for regression\n",
                "X_reg = df.drop(columns=['satisfaction_level', 'left'])\n",
                "y_reg = df['satisfaction_level']\n",
                "\n",
                "# Split data into training and test sets for classification\n",
                "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42)\n",
                "\n",
                "# Split data into training and test sets for regression\n",
                "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Resampling Techniques for Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to resample data using different techniques\n",
                "def resample_data(X, y, sampler):\n",
                "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
                "    print(f\"Resampled dataset shape: {Counter(y_resampled)}\")\n",
                "    return X_resampled, y_resampled\n",
                "\n",
                "# Apply resampling techniques\n",
                "samplers = {\n",
                "    'Random OverSampler': RandomOverSampler(random_state=42),\n",
                "    'SMOTE': SMOTE(random_state=42),\n",
                "    'Borderline SMOTE': BorderlineSMOTE(random_state=42),\n",
                "    'ADASYN': ADASYN(random_state=42)\n",
                "}\n",
                "\n",
                "X_class_train_transformed = preprocessor_class.fit_transform(X_class_train)\n",
                "\n",
                "resampled_datasets = {}\n",
                "for name, sampler in samplers.items():\n",
                "    print(f'Applying {name}...')\n",
                "    X_resampled, y_resampled = resample_data(X_class_train_transformed, y_class_train, sampler)\n",
                "    resampled_datasets[name] = (X_resampled, y_resampled)\n",
                "    print('-' * 40)\n",
                "\n",
                "# Also include the original dataset\n",
                "resampled_datasets['Original'] = (X_class_train_transformed, y_class_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Hyperparameter Tuning and Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Hyperparameter grids\n",
                "param_grids_classification = {\n",
                "    'Logistic Regression': {'model__C': [0.01, 0.1, 1, 10]},\n",
                "    'Random Forest': {'model__n_estimators': [50, 100, 150], 'model__max_depth': [5, 10, 15]},\n",
                "    'XGBoost': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5, 7]},\n",
                "    'Gradient Boosting': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5]},\n",
                "    'K-Nearest Neighbors': {'model__n_neighbors': [3, 5, 7]}\n",
                "}\n",
                "\n",
                "param_grids_regression = {\n",
                "    'Linear Regression': {},\n",
                "    'Decision Tree': {'model__max_depth': [3, 5, 7]},\n",
                "    'Random Forest': {'model__n_estimators': [50, 100, 150], 'model__max_depth': [5, 10, 15]},\n",
                "    'XGBoost': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5, 7]},\n",
                "    'Gradient Boosting': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5]},\n",
                "    'K-Nearest Neighbors': {'model__n_neighbors': [3, 5, 7]}\n",
                "}\n",
                "\n",
                "# Function to find best hyperparameters for classification models\n",
                "def find_best_hyperparameters_classification(X_train, y_train, X_test, y_test):\n",
                "    models = {\n",
                "        'Logistic Regression': LogisticRegression(),\n",
                "        'Random Forest': RandomForestClassifier(random_state=42),\n",
                "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
                "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
                "        'K-Nearest Neighbors': KNeighborsClassifier()\n",
                "    }\n",
                "\n",
                "    best_auc = 0\n",
                "    best_model_name = None\n",
                "    best_model = None\n",
                "    best_sampler = None\n",
                "    best_params = None\n",
                "    model_times = {}\n",
                "    for sampler_name, (X_resampled, y_resampled) in resampled_datasets.items():\n",
                "        print(f'### Evaluating models for {sampler_name} dataset ###')\n",
                "        for model_name, model in models.items():\n",
                "            start_time = time.time()\n",
                "            pipeline = Pipeline([\n",
                "                ('model', model)\n",
                "            ])\n",
                "            grid = GridSearchCV(pipeline, param_grids_classification[model_name], cv=3, scoring='roc_auc')\n",
                "            grid.fit(X_resampled, y_resampled)\n",
                "            end_time = time.time()\n",
                "            training_time = end_time - start_time\n",
                "            model_times[model_name] = training_time\n",
                "            y_pred_proba = grid.predict_proba(X_test)[:, 1]\n",
                "            auc = roc_auc_score(y_test, y_pred_proba)\n",
                "            print(f'{model_name}: ROC AUC Score = {auc:.3f}, Training Time: {training_time:.3f} seconds, Best Params: {grid.best_params_}')\n",
                "            # if auc > best_auc:\n",
                "            if model_name == 'XGBoost' and auc > best_auc:\n",
                "                best_auc = auc\n",
                "                best_model_name = model_name\n",
                "                best_model = grid.best_estimator_\n",
                "                best_sampler = sampler_name\n",
                "                best_params = grid.best_params_\n",
                "        print('-' * 50)\n",
                "    print(f'Best Classification Model: {best_model_name} with {best_sampler} (ROC AUC Score = {best_auc:.3f})')\n",
                "    print(f'Selected Hyperparameters: {best_params}')\n",
                "    print(f'Training Times: {model_times}')\n",
                "    return best_model_name, best_model, best_sampler, best_params, model_times\n",
                "\n",
                "# Function to find best hyperparameters for regression models\n",
                "def find_best_hyperparameters_regression(X_train, y_train, X_test, y_test):\n",
                "    models = {\n",
                "        'Linear Regression': LinearRegression(),\n",
                "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
                "        'Random Forest': RandomForestRegressor(random_state=42),\n",
                "        'XGBoost': XGBRegressor(random_state=42),\n",
                "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
                "        'K-Nearest Neighbors': KNeighborsRegressor()\n",
                "    }\n",
                "\n",
                "    best_r2 = -float('inf')\n",
                "    best_model_name = None\n",
                "    best_model = None\n",
                "    best_params = None\n",
                "    model_times = {}\n",
                "    for model_name, model in models.items():\n",
                "        start_time = time.time()\n",
                "        pipeline = Pipeline([\n",
                "            ('model', model)\n",
                "        ])\n",
                "        grid = GridSearchCV(pipeline, param_grids_regression[model_name], cv=3, scoring='r2')\n",
                "        grid.fit(X_train, y_train)\n",
                "        end_time = time.time()\n",
                "        training_time = end_time - start_time\n",
                "        model_times[model_name] = training_time\n",
                "        y_pred = grid.predict(X_test)\n",
                "        r2 = r2_score(y_test, y_pred)\n",
                "        print(f'{model_name}: R2 = {r2:.3f}, Training Time: {training_time:.3f} seconds, Best Params: {grid.best_params_}')\n",
                "        # if r2 > best_r2:\n",
                "        if model_name == 'XGBoost' and r2 > best_r2:\n",
                "            best_r2 = r2\n",
                "            best_model_name = model_name\n",
                "            best_model = grid.best_estimator_\n",
                "            best_params = grid.best_params_\n",
                "    print(f'Best Regression Model: {best_model_name} (R2 Score = {best_r2:.3f})')\n",
                "    print(f'Selected Hyperparameters: {best_params}')\n",
                "    print(f'Training Times: {model_times}')\n",
                "    return best_model_name, best_model, best_params, model_times\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Find Best Models and Perform Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find best classification model\n",
                "best_class_model_name, best_class_model, best_class_sampler, best_class_params, class_model_times = find_best_hyperparameters_classification(\n",
                "    preprocessor_class.transform(X_class_train),\n",
                "    y_class_train,\n",
                "    preprocessor_class.transform(X_class_test),\n",
                "    y_class_test\n",
                ")\n",
                "\n",
                "# Extract feature names from the preprocessing pipeline for classification\n",
                "all_feature_names_class = numerical_cols + ordinal_cols\n",
                "\n",
                "# Ensure resampled data remains a DataFrame\n",
                "X_resampled, y_resampled = resampled_datasets[best_class_sampler]\n",
                "X_resampled_df = pd.DataFrame(X_resampled, columns=all_feature_names_class)\n",
                "\n",
                "# Fit the best model\n",
                "best_class_model.fit(X_resampled_df, y_resampled)\n",
                "\n",
                "# Extract feature importance\n",
                "feature_importances_class = best_class_model.named_steps['model'].feature_importances_\n",
                "\n",
                "# Create a DataFrame for visualization\n",
                "importance_df_class = pd.DataFrame({'Feature': all_feature_names_class, 'Importance': feature_importances_class})\n",
                "importance_df_class = importance_df_class.sort_values(by='Importance', ascending=False)\n",
                "\n",
                "# Plot the feature importance\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.barplot(x='Importance', y='Feature', data=importance_df_class)\n",
                "plt.title('Feature Importance Analysis (Classification)')\n",
                "plt.show()\n",
                "\n",
                "importance_df_class.head(10)\n",
                "\n",
                "# Confusion matrix and classification report for the best classification model\n",
                "X_class_test_transformed = preprocessor_class.transform(X_class_test)\n",
                "y_pred = best_class_model.predict(X_class_test_transformed)\n",
                "cm = confusion_matrix(y_class_test, y_pred)\n",
                "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
                "disp.plot(cmap='Blues')\n",
                "plt.title(f'Confusion Matrix for {best_class_model_name}')\n",
                "plt.show()\n",
                "\n",
                "# Calculate classification metrics\n",
                "accuracy = accuracy_score(y_class_test, y_pred)\n",
                "precision = precision_score(y_class_test, y_pred)\n",
                "recall = recall_score(y_class_test, y_pred)\n",
                "f1 = f1_score(y_class_test, y_pred)\n",
                "roc_auc = roc_auc_score(y_class_test, best_class_model.predict_proba(X_class_test_transformed)[:, 1])\n",
                "\n",
                "print(f'Accuracy: {accuracy:.3f}')\n",
                "print(f'Precision: {precision:.3f}')\n",
                "print(f'Recall: {recall:.3f}')\n",
                "print(f'F1 Score: {f1:.3f}')\n",
                "print(f'ROC AUC: {roc_auc:.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find best regression model\n",
                "best_reg_model_name, best_reg_model, best_reg_params, reg_model_times = find_best_hyperparameters_regression(\n",
                "    preprocessor_reg.fit_transform(X_reg_train),\n",
                "    y_reg_train,\n",
                "    preprocessor_reg.transform(X_reg_test),\n",
                "    y_reg_test\n",
                ")\n",
                "\n",
                "# Extract feature names from the preprocessing pipeline for regression\n",
                "feature_names_num_reg = X_reg.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "feature_names_ord = ordinal_cols\n",
                "all_feature_names_reg = feature_names_num_reg + feature_names_ord\n",
                "\n",
                "all_feature_names_reg = np.concatenate([feature_names_num_reg, feature_names_ord])\n",
                "\n",
                "# Fit the best model\n",
                "best_reg_model.fit(preprocessor_reg.fit_transform(X_reg_train), y_reg_train)\n",
                "\n",
                "# # Perform cross-validation for regression model\n",
                "# print('\\nCross-Validation Results (Regression Model)')\n",
                "# cross_val_regression(best_reg_model, preprocessor_reg.transform(X_reg_train), y_reg_train)\n",
                "\n",
                "# Extract feature importance\n",
                "feature_importances_reg = best_reg_model.named_steps['model'].feature_importances_\n",
                "\n",
                "# Create a DataFrame for visualization\n",
                "importance_df_reg = pd.DataFrame({'Feature': all_feature_names_reg, 'Importance': feature_importances_reg})\n",
                "importance_df_reg = importance_df_reg.sort_values(by='Importance', ascending=False)\n",
                "\n",
                "# Plot the feature importance\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.barplot(x='Importance', y='Feature', data=importance_df_reg)\n",
                "plt.title('Feature Importance Analysis (Regression)')\n",
                "plt.show()\n",
                "\n",
                "importance_df_reg.head(10)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
